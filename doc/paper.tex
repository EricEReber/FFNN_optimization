\documentclass[onecolumn,10pt,cleanfoot]{asme2ej}

\usepackage{graphicx} %% for loading jpg figures
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{listings}
\usepackage{tablefootnote}
\usepackage{float}
\usepackage{xcolor}
\usepackage{xurl}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks%  save the current one
  \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
  \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
  \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
  \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
  \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
  \do\Y\do\Z}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{breaklines=true, style=mystyle}
%% The class has several options
%  onecolumn/twocolumn - format for one or two columns per page
%  10pt/11pt/12pt - use 10, 11, or 12 point font
%  oneside/twoside - format for oneside/twosided printing
%  final/draft - format for final/draft copy
%  cleanfoot - take out copyright info in footer leave page number
%  cleanhead - take out the conference banner on the title page
%  titlepage/notitlepage - put in titlepage or leave out titlepage
%  
%% The default is oneside, onecolumn, 10pt, final

\title{Neural networks for solving regression and classification problems}

%%% first author
\author{Jonatan H. Hanssen
    \affiliation{
	Bachelor Student, Robotics and \\
	Intelligent Systems\\ \\[-10pt]
	Department of Informatics\\ \\[-10pt]
	The faculty of Mathematics and \\
	Natural Sciences\\ \\[-10pt]
    Email: jonatahh@ifi.uio.no
    }
}

\author{Eric E. Reber
    \affiliation{
	Bachelor Student, Robotics and \\
	Intelligent Systems\\ \\[-10pt]
	Department of Informatics\\ \\[-10pt]
	The faculty of Mathematics and \\
	Natural Sciences\\ \\[-10pt]
    Email: ericer@ifi.uio.no
    }
}

\author{Gregor Kajda
    \affiliation{
	Bachelor Student, Robotics and \\
	Intelligent Systems\\ \\[-10pt]
	Department of Informatics\\ \\[-10pt]
	The faculty of Mathematics and \\
	Natural Sciences\\ \\[-10pt]
    Email: grzegork@ifi.uio.no
    }
}


\begin{document}


\maketitle



\section{Abstract}

We developed a Feed Forward Neural Network (FFNN) and applied it to many common datasets used for benchmarking machine learning algorithms. Furthermore, we compared our results to those attained by linear and logistic regression. To optimize our network, we explored the use of different schedulers in our gradient descent method, and compared their performance against eachother. Overall, we found that our FFNN performed well in all classification tasks we tested, attaining test accuracies between $95$ and $100$ percent for all classification problems. However, we also found that simple logistic regression performed similarly, with a much lower computational cost. For regression on real terrain data, we found that our neural network slightly outperformed our previous polynomial fit, attaining an MSE of MONKEY, PERCENT percent lower than simple linear regression. In our gradient descent, we found that SCHEDULER performed best, being 1 BILLION percent better than the average of all other methods.

%we found our neural net lacking, not being able to beat our polynomial fit from our previous project, and only attaining an MSE of 1 BILLION, which is 2000 BILLION lower than our best polynomial fit of the Franke Function. 


% \tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{nomenclature}
% \entry{A}{You may include nomenclature here.}
% \entry{$\alpha$}{There are two arguments for each entry of the nomemclature environment, the symbol and the definition.}
% \end{nomenclature}
%
% The primary text heading is  boldface and flushed left with the left margin.  The spacing between the  text and the heading is two line spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Neural Networks are all the rage these days, everyone wants to have a neural network in their home. Ive even seen people install neural networks in their dogs. \cite{ericjona}

% Neural networks are powerful tools, able to solve many different problems with 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Method}

\subsection{Theory}

This sections covers the theoretic background which underlies the methods used in this paper.

\subsubsection{Logistic Regression}

As part of the research conducted in this project, we introduce a new type of regression model, known as Logistic regression. This is a powerful, yet simple algorithm used in machine learning for the purpose of approximating a probability associated with the occurrence of a specific event given measured data. The fundamental idea behind logistic regression is based on the exact approach used in Linear Regression, meaning that we wish to describe a relationship between dependent and explanatory variables in terms of a set of estimated parameters. However, in this case we are not using the weighted sum of our basis functions directly, but feeding them into an activation function. This function, often the sigmoid function, maps the reals onto a range $y \in (0,1)$. This in turn allows for the computation of probability related to an event taking place. Rounding of said probability leads to labeling of each input as belonging to one of two classes, meaning that each object fed into the model will be classified as being of class 0 or 1 (in the case of binary classification).

% To begin with however, the probability output by the activation function will be incorrect for majority of inputs, due to the random nature of the initialized parameters. While this may seem to be an issue, the weights can be adjusted to give better classification results through the application of the gradient descent coupled with an adjustable variable called learning rate. As we also shall see, the implementation of an activation function at the output gives a significant advantage over the classic Linear regression in terms of classification power.
 

\subsubsection{Gradient Descent}

In our previous project, we have primarily dealt with problems that have an analytical solution. This means that once a model has been decided on (polynomial degree and value for lambda for example), finding the optimal solution is a deterministic process which does not require further tuning. However, for the methods we use in this project - such as Logistic Regression - no analytical solutions exist, thus we will have to turn to numerical techniques to find the 'ideal' solution. Our method of choice in this case is gradient descent, an optimization technique which reduces the residuals by utilizing the idea of moving downhill a convex function until a globally minimal scalar value is reached. The gradient itself is nothing more than a first order derivative of a given function, which points in the direction of the greatest change. 

The application of gradient descent to our problems as means of finding the ideal solution implies that the parameters be initialized randomly. The most common approach is to set each parameter equal to a normal distribution with mean zero and variance equal to one, and iteratively update the parameters by calculating the gradient at each given point in the residual space \cite[preface p. x]{dds}. For any one-layer model, the gradient function used to tune the weights will take the form of the product of the partial derivative of the cost function with respect to the activation function, and the partial derivative of the activation function with respect to the weights: 

\begin{equation}
\frac{\partial{\cal C}}{\partial w_{jk}}  = \frac{\partial C}{\partial a_{j}}\frac{\partial a_j}{\partial w_{jk}}
\end{equation}

Although gradient decent is an excellent algorithm for finding optimal solutions, it can also become quite computationally expensive for problems where large numbers of datapoints are available. However, this issue may be effectively countered by applying a variant of standard gradient approach known as Stochastic Gradient Decent or simply SGD. The SGD is based on randomly choosing one or a small number of instances from the dataset to update the parameters, hence the name stochastic. While it may seem like a poor idea to approximate the gradient instead of using the true gradient, the SGD has proven to give a large computational advantage and will often allow the model to converge into a global minima faster than GD. Furthermore, the stochastic nature of this method can allow SGD to avoid getting stuck in local minima \cite[46]{sr}.

\subsubsection{Schedulers}

Whilst SGD itself gives a significant boost to the learning pace of machine learning models, it is possible to improve the algorithm further by performing what is known as scheduling. These methods work by adaptively adjusting the learning rate, thus allowing us to avoid local minima and converge even faster. One of the simpler methods involves adding a fraction of the previous change to the current value used to update the weights, thus keeping a sort of memory of the previous iterations. This is called momentum, due to the physical analogue of a moving particle having momentum. With this addition, we will keep moving in parameter space even if our current gradient is small, because we have gained momentum in a certain direction based on our previous iterations. Furthermore, we will keep moving in the direction where most gradients point, even if outliers point in other directions. Other, more complex methods, also keep track of approximations of the second moment of the cost function. These methods aim to adaptively change the learning rate for different directions in parameter space, so that small steps are taking in steep, narrow directions and large steps taken in shallow, flat directions \cite{mortensched}. Examples of schedulers using the second moment are RMS-prop, Adam and Adagrad.

% Adaptive optimization methods don't always have good generalization. Recent studies have shown that adaptive methods such as ADAM, RMSPorp, and AdaGrad tend to have poor generalization compared to SGD or SGD with momentum, particularly in the high-dimensional limit (i.e. the number of parameters exceeds the number of data points). Although it is not clear at this stage why these methods perform so well in training deep neural networks, simpler procedures like properly-tuned SGD may work as well or better in these applications.

\subsubsection{Feedforward Neural Networks}

For some problems, like approximating a simple function like the Franke Function in project 1, it is sufficient to choose a set of basis functions and find their optimal linear combination. Given that our problem can be reasonably approximated by this basis, we can achieve good results. However, by limiting ourselves to linear functions, we are possibly limiting our model, as our dataset may well be better approximated by a different basis, our perhaps not even by a linear combination of functions at all. Because of this, linear regression is not able to understand the interaction between two arbitrary input variables \cite[165]{gbc}. A better approach would be to simply feed our model our features directly, and hope that it may somehow learn the relationship organically.

To learn any arbitrary relationship between our features may at first seem like an intractable problem, but we are actually able to achieve this with relatively small changes to our linear regression methods. Instead of having our prediction vector be a linear combination of our input vector, we use the linear combinations of the inputs to create an intermediate vector, and apply an activation function to this vector. The resulting values from this function are either passed to the output as a linear combination again, or sent to another intermediate vector. By having the activation function be a nonlinear function, such as the sigmoid or RELU function, we are able to approximate any continous function to arbitrary accuracy \cite[230]{cmb}. By introducing these intermediate vectors, we have created a layered structure, where information is fed forward from layer to layer. Because of this, we call this model a Multilayer Perceptron or a Feedforward Neural Network.

Mathematically, each node on a layer is a linear combination of all the nodes of the previous layer, fed into the activation function for the current layer.

\begin{equation}
a_i^l = f^l(z_i^l) = f^l\left(\sum_{j=1}^{N_{l-1}} w_{ij}^l a_j^{l-1} + b_i^l\right).
\end{equation}

% For an FFNN with an arbitrary amount of hidden layers, the output layer is thus defined as
%
% \begin{equation}
% a^{l+1}_i = f^{l+1}\left[\!\sum_{j=1}^{N_l} w_{ij}^3 f^l\left(\sum_{k=1}^{N_{l-1}}w_{jk}^{l-1}\left(\dots f^1\left(\sum_{n=1}^{N_0} w_{mn}^1 x_n+ b_m^1\right)\dots\right)+b_k^2\right)+b_1^3\right].
% \end{equation}
%
% \begin{equation}
% 	a^{L}_i = f^{L}\left[\!\sum_{j=1}^{N_{L-1}} w_{ij}^3 f^{L-1}\left(\sum_{k=1}^{N_{L-2}}w_{jk}^{L-2}\left(\dots f^1\left(\sum_{n=1}^{N_0} w_{mn}^1 x_n+ b_m^1\right)\dots\right)+b_k^2\right)+b_1^3\right].
% \end{equation}

\subsubsection{Activation functions}

As we have seen, the behaviour of our neural network is defined by the amount of hidden layers and the amount of nodes in each one (our architecture), but also by our choice of activation functions. For the output layer, the activation is usually easily decided based on the problem (identity for regression problems, sigmoid for binary classification, SoftMax for multiclass). For the hidden layers, the choice of activation function is often decided through a process of trial and error, as this is an area without definitive guiding theoretical principles \cite[188]{gbc}. Below are the definitions of some of the most common activation functions.

The sigmoid is defined as follows:

\begin{equation}
y = \frac{1}{1 + exp(x)}
\end{equation}

As we can see from the equation, this function has a range of $y \in (0,1)$, and for high values of x, the rate of change is very small. This saturation is a problem for gradient descent methods, and especially for backpropagation, as the repeated multiplication of very small gradients can lead to weights barely being updated and training slowing down (or even stopping completely). Because of this saturation, the sigmoid is generally not used as an activation function for the hidden layers \cite[191]{gbc}.

Another activation function is the Rectified Linear Unit, and its cousin, the Leaky Rectified Linear Unit:

\begin{equation}
RELU(x) = \left\{\begin{array}{cc} 0 & \; \; if \; \; x < 0, \\  x & \; \; if \; \; x \ge 0.\end{array}\right.
\end{equation}

\begin{equation}
LRELU(x) = \left\{\begin{array}{cc} -\alpha \cdot x & \; \; if \; \; x < 0, \\  x & \; \; if \; \; x \ge 0.\end{array}\right.
\end{equation}

These do not suffer the same saturation problems displayed by the sigmoid function, and as such are often the default choice of activation for the hidden layers \cite[188]{gbc}.

\subsubsection{Backpropagation}

Like logistic regression, the optimal weights for each layer in an FFNN can not be found analytically, and we have to turn to gradient descent. By repeated use of the chain rule, we are able to calculate the gradient for each layer iteratively, using the gradient of the layer after it and starting at the output. This is simply the reverse mode of automatic differentiation, and is computationally preferred compared to forward mode, where we start at the input layer \cite[416]{sr}. The equations for calculating the gradients in an FFNN, which we will not derive here, are as follows \cite{morten}:

First we calculate the so called delta terms for all layers, starting with the output layer:

\begin{equation}
y_i^l = f^l(u_i^l) = f^l\left(\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\right).
\end{equation}

We use this term in an iterative process, working backwards through the network with the following equation:

\begin{equation}
\delta_j^l = \sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l).
\end{equation}

Using these delta terms, we can calculate the gradients for the weights and biases in each layer:

\begin{gather}
w_{jk}^l\leftarrow  = w_{jk}^l- \eta \delta_j^la_k^{l-1} \\
b_j^l \leftarrow b_j^l-\eta \frac{\partial {\cal C}}{\partial b_j^l}=b_j^l-\eta \delta_j^l
\end{gather}

With the theory explained, we can go on to describe how we have applied it to our problem.

\subsection{Implementation}

We implement the FFNN as a class, using NumPy's many matrix functions. In the instantiation of the class, one is able to specify an arbitrary architecture, and choose activation functions for the output and the hidden layer. The cost function can also be specified, and this, along with the choice of output function, allows the network to be used for regression, binary classification and multiclass classification. By specifying no hidden layers in the architecture, the model can be used for linear and logistic regression as well. Because of this, no explicit implementation of linear and logistic regression has been made, which means that almost all relevant code is contained in \texttt{src/FFNN.py} and \texttt{src/Schedulers.py}. When instatiating the model, we set the weights to normal distributions with mean zero and variance one. We set the bias to $0.1$ as recommended in Goodfellow et Al. \cite[189]{gbc}.

The feedforward part is implemented as a simple matrix vector multiplication, followed by an elementwise application of the chosen activation function. For processing batches of inputs, the same multiplication code is used, but this time the input is the design matrix instead of a single row. Each hidden layer will now contain matrices themselves, consisting of a number of rows equivalent to the batch size and a number of columns equal to the number of nodes in this layer.

The backpropagation is implemented by differentiating all the functions declared when the class is instatiated, and following the algorithm laid out in Hjorth-Jensens lectures \cite{morten}. For batch backpropagation, we accumulate the gradient. In practice this means that the gradient for the weights, normally a matrix the same size as the relevant weight matrix, is now one matrix per element in our batch (in practice implemented as a NumPy array of three dimensions). At the end of the batch, we average all the gradients for each weight matrix together and feed this into our scheduling calculation to determine how much our weights should change.

Training a model is implemented as repeated calls to feedforward and backpropagation, once per batch per epoch. During training, one is able to specify which scheduler to use. These are implemented as a series of classes all inheriting from a base class, and are all implemented following Hjorth-Jensens lectures \cite{mortensched}. For finding the optimal parameters, we implement our own gridsearch which finds the lowest error among combinations of lambda, eta, and parameters used for the scheduling.

For verification that our implementation is working correctly, we train on a simple second degree polynomial, and see that our predicted y value is very close to the actual y value\footnote{see \texttt{src/verification.py}}. See Fig.~\ref{verification}.

\begin{figure}[h]
\centerline{\includegraphics[width=5in]{figure/verification.png}}
\caption{Simple verification case}
\label{verification}
\end{figure}

\section{Results}

\subsection{Different schedulers used with linear regression}

\subsection{Neural Network for regression}

For the organic terrain data used in our previous project, we found that our neural net performed slightly better, gaining an MSE of NUMBER, which was about PERCENTAGE better than our best polynomial fit. However, looking at our prediction visually, and also comparing the MSE to the one gained on the Franke Function, we see that we are still lacking. For terrain data like this, which contain a lot of spatial information, we believe that Convolutional Neural Networks could be a better fit.

\subsection{Neural Network for binary and multiclass classification}

To test our networks ability to classify data, we trained it on many common datasets used in the development of neural networks. The datasets were the Wisconsin Breast Cancer dataset and the 8x8 MNIST dataset, as well as other datasets\footnote{Sonar dataset, House dataset}. We found good results on all datasets achieving a test accuracy of over 97\% on all 

\subsection{Logistic Regression}

Although our neural network performed well, we also found very similar results with logistic regression, at a much lower computational cost. This implies that our classification datasets were perhaps too simple, and that using Neural Networks was not needed to achieve good results.

% \begin{table}
% \begin{center}
% \label{ols_mse_table_dif_data}
% \begin{tabular}{c l l l}
% Dataset & Std of noise & Best MSE & Best degree \\
% \hline
% Franke & 0 & 0.0005109 & 9\\
% Franke & 0.05 & 0.0038302 & 7 \\
% Franke & 0.10 & 0.0139833 & 7 \\
% Terrain 1 & - & 0.0142683 & 19 \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

% \begin{table*}[t]
% \caption{Best MSE for OLS, Ridge and Lasso regression on real elevation data. We only fit up to degree 9 due to computational constraints}
% \begin{center}
% \label{ols_vs_ridge_vs_lasso_table_real}
% \begin{tabular}{c | l l l}
% Regression & MSE & Degree & $\lambda$ \\
% \hline
% OLS & 0.01538 & 9 & - \\
% Ridge & 0.01538 & 9 & 0 \\
% Lasso & 0.01685 & 9 & $2.154 \cdot 10^{-7}$ \\
% \hline
% \end{tabular}
% \end{center}
% \end{table*}

\section{Conclusion}

In this paper we have explored the use of a Feed Forward Neural Network for both regression and classification. We have studied how different schedulers, 

\bibliographystyle{apalike}
\bibliography{bibliography}

\section*{Appendix A: Plots and tables}

The next pages contain plots and tables that are of interest.

\begin{table*}
\caption{This table shows the command to execute to reproduce every figure in the report. (More info about the scripts and their parameters can be found in the README)}
\begin{center}
\label{allparamstable}
\begin{tabular}{c | l l l}
Figure & Shell command (leading \texttt{python3} omitted) \\
\hline
\ref{frankenonoise} & \texttt{task\_b.py --noise 0 -n 25}\\
\hline
\end{tabular}
\end{center}
\end{table*}

\end{document}
